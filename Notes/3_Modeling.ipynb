{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\">ML Modeling</h1>\n",
    "<h3 align=\"center\">Creating Models with Deep Learning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Nueral Networks \n",
    "- Recurrent Neural Networks (RNNs) – Used for sequential data such as time series and NLP.\n",
    "- Convolutional Neural Networks (CNNs) – Used for image processing and computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Activation Functions\n",
    "These functions define the output of a node / neuron given its input signals. Linear Activation functions arent used. \n",
    "\n",
    "Best practice is to choose a non-linear activation function due to it: \n",
    "- These can create complex mappings between inputs and outputs\n",
    "- Allow backpropagation (because they have a useful derivative)\n",
    "- Allow for multiple layers (linear functions degenerate to a single layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Sigmoid/Logistic Function**\n",
    "- Scales everything from 0 to 1 based on neuron input.  \n",
    "\n",
    " **Tanh / Hyperbolic Tangent Function**\n",
    "- Scales everything from -1 to 1 based on neuron input.  \n",
    "- Generally preferred over sigmoid due to being averaged around 0.  \n",
    "\n",
    " **Cons for Both:**\n",
    "- Suffers from the **vanishing gradient problem**.  \n",
    "- Changes slowly for high or low values.  \n",
    "- Computationally expensive.  \n",
    "\n",
    "---\n",
    "\n",
    " **Rectified Linear Unit (ReLU)**\n",
    "- Most commonly used, very popular choice.  \n",
    "- Easy & fast to compute due to linearity.  \n",
    "\n",
    " **Cons:**\n",
    "- When inputs are zero or negative, we have a linear function and all of its problems (**Dying ReLU problem**).  \n",
    "- Solution: **Leaky ReLU**.  \n",
    "\n",
    "---\n",
    "\n",
    " **Leaky ReLU**\n",
    "- Solves **Dying ReLU** by introducing a negative slope below 0.  \n",
    "- Negative slope is determined arbitrarily.  \n",
    "\n",
    "---\n",
    "\n",
    "**Parametric ReLU (PReLU)**\n",
    "- ReLU, but the slope in the negative part is **learned via backpropagation**.  \n",
    "- More flexible but **computationally expensive**.  \n",
    "\n",
    "---\n",
    "\n",
    "**Other ReLU Variants**\n",
    "**Swish**\n",
    "- Developed by **Google**, performs really well.  \n",
    "- Mostly beneficial for **very deep networks** (40+ layers).  \n",
    "\n",
    "---\n",
    "\n",
    "<table style=\"width: 100%; text-align: center;\">\n",
    "    <tr>\n",
    "        <td><img src=\"../Figures/Modeling/ActivationFuns.png\" style=\"width: 600px;\"></td>\n",
    "        <td><img src=\"../Figures/Modeling/Relu.png\" style=\"width: 600px;\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNS"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
