{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\">ML Modeling</h1>\n",
    "<h3 align=\"center\">Creating Models with Deep Learning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Nueral Networks \n",
    "- Recurrent Neural Networks (RNNs) ‚Äì Used for sequential data such as time series and NLP.\n",
    "- Convolutional Neural Networks (CNNs) ‚Äì Used for image processing and computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Activation Functions\n",
    "These functions define the output of a node / neuron given its input signals. Linear Activation functions arent used. \n",
    "\n",
    "Best practice is to choose a non-linear activation function due to it: \n",
    "- These can create complex mappings between inputs and outputs\n",
    "- Allow backpropagation (because they have a useful derivative)\n",
    "- Allow for multiple layers (linear functions degenerate to a single layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Sigmoid/Logistic Function**\n",
    "- Scales everything from 0 to 1 based on neuron input.  \n",
    "\n",
    " **Tanh / Hyperbolic Tangent Function**\n",
    "- Scales everything from -1 to 1 based on neuron input.  \n",
    "- Generally preferred over sigmoid due to being averaged around 0.  \n",
    "\n",
    " **Cons for Both:**\n",
    "- Suffers from the **vanishing gradient problem**.  \n",
    "- Changes slowly for high or low values.  \n",
    "- Computationally expensive.  \n",
    "\n",
    "---\n",
    "\n",
    " **Rectified Linear Unit (ReLU)**\n",
    "- Most commonly used, very popular choice.  \n",
    "- Easy & fast to compute due to linearity.  \n",
    "\n",
    " **Cons:**\n",
    "- When inputs are zero or negative, we have a linear function and all of its problems (**Dying ReLU problem**).  \n",
    "- Solution: **Leaky ReLU**.  \n",
    "\n",
    "---\n",
    "\n",
    " **Leaky ReLU**\n",
    "- Solves **Dying ReLU** by introducing a negative slope below 0.  \n",
    "- Negative slope is determined arbitrarily.  \n",
    "\n",
    "---\n",
    "\n",
    "**Parametric ReLU (PReLU)**\n",
    "- ReLU, but the slope in the negative part is **learned via backpropagation**.  \n",
    "- More flexible but **computationally expensive**.  \n",
    "\n",
    "---\n",
    "\n",
    "**Other ReLU Variants**\n",
    "**Swish**\n",
    "- Developed by **Google**, performs really well.  \n",
    "- Mostly beneficial for **very deep networks** (40+ layers).  \n",
    "\n",
    "---\n",
    "\n",
    "<table style=\"width: 100%; text-align: center;\">\n",
    "    <tr>\n",
    "        <td><img src=\"../Figures/Modeling/ActivationFuns.png\" style=\"width: 600px;\"></td>\n",
    "        <td><img src=\"../Figures/Modeling/Relu.png\" style=\"width: 600px;\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soft-Max** commonly used as the Last output-layer output \n",
    "- Used on the final output layer of a multi-class classification problem\n",
    "- converts outputs to probabilities of each classification\n",
    "- Can‚Äôt produce more than one label for something, total probability equals 1 so only one label. \n",
    "- (sigmoid can predict more than one label) like (Sigmoid with Binary Cross-Entropy (BCE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNS\n",
    "CNNs excel at spatially structured data like, overall unstructured data: \n",
    "- ‚úÖ Images & Videos ‚Äì Object detection, segmentation, recognition.\n",
    "- ‚úÖ Audio & Spectrograms ‚Äì Converting speech/audio into an image-like format for classification.\n",
    "- ‚úÖ Medical Imaging ‚Äì X-rays, MRIs, CT scans for disease detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How CNNs Work**  \n",
    "CNNs consist of several key layers that transform raw images into meaningful patterns:\n",
    "\n",
    "#### **1Ô∏è‚É£ Convolutional Layer**\n",
    "- Applies **filters (kernels)** that scan the image.\n",
    "- Detects **edges, textures, and shapes**.\n",
    "- Each filter learns different features automatically.\n",
    "\n",
    "#### **2Ô∏è‚É£ Pooling Layer**\n",
    "- **Reduces image size** while keeping essential features.\n",
    "- Helps make computations **faster and efficient**.\n",
    "- Common types: **Max Pooling, Average Pooling**.\n",
    "\n",
    "#### **3Ô∏è‚É£ Fully Connected Layer**\n",
    "- Takes extracted features and makes final predictions.\n",
    "- Works like a traditional neural network to classify data.\n",
    "\n",
    "This is very resource intensive, has lots of hyper-parameters. Main difficulty is getting data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs\n",
    "Recurrent Neural Networks (RNNs) are designed for sequential data processing, making them ideal for tasks like time-series forecasting, natural language processing (NLP), and speech recognition. Unlike traditional neural networks, RNNs have memory, allowing them to capture dependencies over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1Ô∏è‚É£ Recurrent Connections**\n",
    "- Each neuron receives **input from the previous timestep**.\n",
    "- Helps the network **remember past information** while processing new data.\n",
    "- Key feature: **Hidden states**, which store context across timesteps.\n",
    "\n",
    "#### **2Ô∏è‚É£ Backpropagation Through Time (BPTT)**\n",
    "- Special version of **backpropagation** used to update RNN weights.\n",
    "- Unrolls the network over time and **computes gradients across timesteps**.\n",
    "- **Problem:** Can lead to **vanishing gradients**, making training difficult.\n",
    "\n",
    "#### **3Ô∏è‚É£ Variants of RNNs**\n",
    "To overcome RNN limitations, several advanced architectures have been developed:\n",
    "- **LSTMs (Long Short-Term Memory)** ‚Üí Adds **gates (forget, input, output)** to control memory.\n",
    "- **GRUs (Gated Recurrent Units)** ‚Üí Similar to LSTMs but with fewer parameters, making them more efficient.\n",
    "- **Bidirectional RNNs** ‚Üí Processes data **both forward and backward**, capturing more context.\n",
    "\n",
    "---\n",
    "\n",
    "### **Challenges of RNNs**\n",
    "üîπ **Vanishing & Exploding Gradients** ‚Äì Makes long-range dependencies difficult to learn.  \n",
    "üîπ **Computationally Expensive** ‚Äì Due to sequential processing, harder to parallelize than CNNs.  \n",
    "üîπ **Lots of Hyperparameters** ‚Äì Requires tuning learning rates, hidden units, sequence lengths.  \n",
    "üîπ **Data Dependency** ‚Äì Needs **large labeled datasets** to generalize well.  \n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use RNNs?**\n",
    "‚úÖ **Text Processing** ‚Äì Language modeling, chatbots, machine translation.  \n",
    "‚úÖ **Time-Series Analysis** ‚Äì Stock market predictions, sensor data analysis.  \n",
    "‚úÖ **Speech & Audio** ‚Äì Speech recognition, music generation.  \n",
    "\n",
    "However, **Transformers (BERT, GPT, ViTs) are replacing RNNs** in most NLP & vision tasks due to their efficiency and scalability.\n",
    "\n",
    "Would you like a **comparison between RNNs, LSTMs, GRUs, and Transformers**? üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Neural Networks\n",
    "Tuning Hyper-parameters of a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "\n",
    "- Neural networks are trained using gradient descent (or similar methods).\n",
    "- Training involves starting at a random point and sampling different solutions (weights) to minimize a cost function over many epochs.\n",
    "- The learning rate determines how far apart these samples are.\n",
    "\n",
    "### Effect of Learning Rate\n",
    "\n",
    "- **Too high**: May overshoot the optimal solution.\n",
    "- **Too low**: Training takes too long to converge.\n",
    "- Learning rate is a **hyperparameter** that requires tuning.\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "- Defines the number of training samples used in each batch of each epoch.\n",
    "\n",
    "### Effects of Batch Size\n",
    "\n",
    "- **Smaller batch sizes**:\n",
    "  - Help escape local minima.\n",
    "  - Can make training appear inconsistent across runs due to random shuffling.\n",
    "- **Larger batch sizes**:\n",
    "  - Can get stuck in a suboptimal solution.\n",
    "\n",
    "### Recap (Important!)\n",
    "\n",
    "- **Small batch sizes** help avoid local minima.\n",
    "- **Large batch sizes** risk converging on the wrong solution.\n",
    "- **Large learning rates** may overshoot the correct solution.\n",
    "- **Small learning rates** slow down training.\n",
    "\n",
    "---\n",
    "\n",
    "_Source: Sundog Education, DataCumulus (¬© 2022)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Leanring\n",
    "Also know as Fine-Tuning Pre-Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is a powerful technique where a pre-trained model is used as a starting point for a new task, saving time and computational resources. Well known Model Zoo or model collection is called Hugging Face. This is a platform that allows users access to numerous open sourced machine learning models. \n",
    "\n",
    "---\n",
    "\n",
    "#### **Approaches to Transfer Learning**\n",
    "##### **1Ô∏è‚É£ Fine-Tuning a Pre-Trained Model**\n",
    "‚úÖ Continue training a **pre-trained model** on new data.  \n",
    "‚úÖ Ideal when the model has been trained on **far more data** than you have.  \n",
    "‚úÖ **Use a low learning rate** to **incrementally** improve performance.  \n",
    "‚úÖ **Freeze lower layers** and add **new trainable layers** to adapt to new tasks.  \n",
    "‚úÖ The model **learns to repurpose old features** for new predictions.  \n",
    "‚úÖ Hybrid approach: **First freeze layers ‚Üí Then fine-tune everything**.\n",
    "\n",
    "---\n",
    "\n",
    "##### **2Ô∏è‚É£ Retraining from Scratch**\n",
    "‚úÖ **Only do this if you have LOTS of training data**.  \n",
    "‚úÖ Data must be **very different** from the original pre-trained dataset.  \n",
    "‚úÖ Requires **high computing power** (e.g., TPUs, GPUs).  \n",
    "‚úÖ Used in cases where pre-trained knowledge **isn‚Äôt relevant**.\n",
    "\n",
    "---\n",
    "\n",
    "##### **3Ô∏è‚É£ Using a Pre-Trained Model \"As-Is\"**\n",
    "‚úÖ If the model was trained on **exactly the data you need**, no extra training is required.  \n",
    "‚úÖ Example: Using a pre-trained **ResNet** for generic **image classification**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üöÄ Choosing the Right Transfer Learning Strategy**\n",
    "| **Scenario** | **Best Approach** |\n",
    "|-------------|------------------|\n",
    "| Small dataset, similar to pre-trained model | ‚úÖ Fine-tune existing model |\n",
    "| Small dataset, very different from pre-trained model | ‚úÖ Add new layers, then fine-tune |\n",
    "| Large dataset, different from pre-trained model | üî• Retrain from scratch |\n",
    "| Model already fits your needs | ‚úÖ Use pre-trained model as-is |\n",
    "\n",
    "---\n",
    "\n",
    "### **üí° TL;DR**\n",
    "- **Fine-tuning**: Best for leveraging pre-trained knowledge while adapting to new tasks.  \n",
    "- **Retrain from scratch**: Only if you have **tons of data** and **high computational power**.  \n",
    "- **Use as-is**: If the model already fits your requirements.  \n",
    "\n",
    "üöÄ **Transfer learning saves time, improves performance, and reduces training costs!**  \n",
    "Would you like an example **implementation in TensorFlow or PyTorch?** üòä\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Regularization Techniques\n",
    "Regularization is a technique used to prevent overfitting, which occurs when a model performs well on training data but struggles with new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Overfitting happens when the model learns patterns that exist only in the training set rather than general trends applicable to real-world data. A key indicator of overfitting is high accuracy on the training set but lower accuracy on the test or evaluation set. Regularization helps address this by discouraging the model from relying too heavily on specific details in the training data.\n",
    " Regulization is used to prevent overfitting. \n",
    "\n",
    " In a neural network overfitting might be due to too many layers and neurons, thus we can prune it and try to use a simpler model. \n",
    " \n",
    " Another techinque is called a drop out layerSo by dropping out specific neurons that are chosen at random, at each training step, we're basically forcing the learning to spread itself out more. And this has the effect of preventing any individual neuron\n",
    " from overfitting to a specific data point, right?  network can make it actually trained better.\n",
    "\n",
    "Another techinque is called early stopping which stops training once overfitting is detected when accuracy goes over the validation accuracy.  \n",
    "\n",
    "L1 Regularization \n",
    "L2 Regurlarization "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
