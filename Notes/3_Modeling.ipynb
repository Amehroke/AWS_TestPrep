{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\">ML Modeling</h1>\n",
    "<h3 align=\"center\">Creating Models with Deep Learning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Nueral Networks \n",
    "- Recurrent Neural Networks (RNNs) ‚Äì Used for sequential data such as time series and NLP.\n",
    "- Convolutional Neural Networks (CNNs) ‚Äì Used for image processing and computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Activation Functions\n",
    "These functions define the output of a node / neuron given its input signals. Linear Activation functions arent used. \n",
    "\n",
    "Best practice is to choose a non-linear activation function due to it: \n",
    "- These can create complex mappings between inputs and outputs\n",
    "- Allow backpropagation (because they have a useful derivative)\n",
    "- Allow for multiple layers (linear functions degenerate to a single layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Sigmoid/Logistic Function**\n",
    "- Scales everything from 0 to 1 based on neuron input.  \n",
    "\n",
    " **Tanh / Hyperbolic Tangent Function**\n",
    "- Scales everything from -1 to 1 based on neuron input.  \n",
    "- Generally preferred over sigmoid due to being averaged around 0.  \n",
    "\n",
    " **Cons for Both:**\n",
    "- Suffers from the **vanishing gradient problem**.  \n",
    "- Changes slowly for high or low values.  \n",
    "- Computationally expensive.  \n",
    "\n",
    "---\n",
    "\n",
    " **Rectified Linear Unit (ReLU)**\n",
    "- Most commonly used, very popular choice.  \n",
    "- Easy & fast to compute due to linearity.  \n",
    "\n",
    " **Cons:**\n",
    "- When inputs are zero or negative, we have a linear function and all of its problems (**Dying ReLU problem**).  \n",
    "- Solution: **Leaky ReLU**.  \n",
    "\n",
    "---\n",
    "\n",
    " **Leaky ReLU**\n",
    "- Solves **Dying ReLU** by introducing a negative slope below 0.  \n",
    "- Negative slope is determined arbitrarily.  \n",
    "\n",
    "---\n",
    "\n",
    "**Parametric ReLU (PReLU)**\n",
    "- ReLU, but the slope in the negative part is **learned via backpropagation**.  \n",
    "- More flexible but **computationally expensive**.  \n",
    "\n",
    "---\n",
    "\n",
    "**Other ReLU Variants**\n",
    "**Swish**\n",
    "- Developed by **Google**, performs really well.  \n",
    "- Mostly beneficial for **very deep networks** (40+ layers).  \n",
    "\n",
    "---\n",
    "\n",
    "<table style=\"width: 100%; text-align: center;\">\n",
    "    <tr>\n",
    "        <td><img src=\"../Figures/Modeling/ActivationFuns.png\" style=\"width: 600px;\"></td>\n",
    "        <td><img src=\"../Figures/Modeling/Relu.png\" style=\"width: 600px;\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soft-Max** commonly used as the Last output-layer output \n",
    "- Used on the final output layer of a multi-class classification problem\n",
    "- converts outputs to probabilities of each classification\n",
    "- Can‚Äôt produce more than one label for something, total probability equals 1 so only one label. \n",
    "- (sigmoid can predict more than one label) like (Sigmoid with Binary Cross-Entropy (BCE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNS\n",
    "CNNs excel at spatially structured data like, overall unstructured data: \n",
    "- ‚úÖ Images & Videos ‚Äì Object detection, segmentation, recognition.\n",
    "- ‚úÖ Audio & Spectrograms ‚Äì Converting speech/audio into an image-like format for classification.\n",
    "- ‚úÖ Medical Imaging ‚Äì X-rays, MRIs, CT scans for disease detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How CNNs Work**  \n",
    "CNNs consist of several key layers that transform raw images into meaningful patterns:\n",
    "\n",
    "#### **1Ô∏è‚É£ Convolutional Layer**\n",
    "- Applies **filters (kernels)** that scan the image.\n",
    "- Detects **edges, textures, and shapes**.\n",
    "- Each filter learns different features automatically.\n",
    "\n",
    "#### **2Ô∏è‚É£ Pooling Layer**\n",
    "- **Reduces image size** while keeping essential features.\n",
    "- Helps make computations **faster and efficient**.\n",
    "- Common types: **Max Pooling, Average Pooling**.\n",
    "\n",
    "#### **3Ô∏è‚É£ Fully Connected Layer**\n",
    "- Takes extracted features and makes final predictions.\n",
    "- Works like a traditional neural network to classify data.\n",
    "\n",
    "This is very resource intensive, has lots of hyper-parameters. Main difficulty is getting data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs\n",
    "Recurrent Neural Networks (RNNs) are designed for sequential data processing, making them ideal for tasks like time-series forecasting, natural language processing (NLP), and speech recognition. Unlike traditional neural networks, RNNs have memory, allowing them to capture dependencies over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1Ô∏è‚É£ Recurrent Connections**\n",
    "- Each neuron receives **input from the previous timestep**.\n",
    "- Helps the network **remember past information** while processing new data.\n",
    "- Key feature: **Hidden states**, which store context across timesteps.\n",
    "\n",
    "#### **2Ô∏è‚É£ Backpropagation Through Time (BPTT)**\n",
    "- Special version of **backpropagation** used to update RNN weights.\n",
    "- Unrolls the network over time and **computes gradients across timesteps**.\n",
    "- **Problem:** Can lead to **vanishing gradients**, making training difficult.\n",
    "\n",
    "#### **3Ô∏è‚É£ Variants of RNNs**\n",
    "To overcome RNN limitations, several advanced architectures have been developed:\n",
    "- **LSTMs (Long Short-Term Memory)** ‚Üí Adds **gates (forget, input, output)** to control memory.\n",
    "- **GRUs (Gated Recurrent Units)** ‚Üí Similar to LSTMs but with fewer parameters, making them more efficient.\n",
    "- **Bidirectional RNNs** ‚Üí Processes data **both forward and backward**, capturing more context.\n",
    "\n",
    "---\n",
    "\n",
    "### **Challenges of RNNs**\n",
    "üîπ **Vanishing & Exploding Gradients** ‚Äì Makes long-range dependencies difficult to learn.  \n",
    "üîπ **Computationally Expensive** ‚Äì Due to sequential processing, harder to parallelize than CNNs.  \n",
    "üîπ **Lots of Hyperparameters** ‚Äì Requires tuning learning rates, hidden units, sequence lengths.  \n",
    "üîπ **Data Dependency** ‚Äì Needs **large labeled datasets** to generalize well.  \n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use RNNs?**\n",
    "‚úÖ **Text Processing** ‚Äì Language modeling, chatbots, machine translation.  \n",
    "‚úÖ **Time-Series Analysis** ‚Äì Stock market predictions, sensor data analysis.  \n",
    "‚úÖ **Speech & Audio** ‚Äì Speech recognition, music generation.  \n",
    "\n",
    "However, **Transformers (BERT, GPT, ViTs) are replacing RNNs** in most NLP & vision tasks due to their efficiency and scalability.\n",
    "\n",
    "Would you like a **comparison between RNNs, LSTMs, GRUs, and Transformers**? üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Neural Networks\n",
    "Tuning Hyper-parameters of a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "\n",
    "- Neural networks are trained using gradient descent (or similar methods).\n",
    "- Training involves starting at a random point and sampling different solutions (weights) to minimize a cost function over many epochs.\n",
    "- The learning rate determines how far apart these samples are.\n",
    "\n",
    "### Effect of Learning Rate\n",
    "\n",
    "- **Too high**: May overshoot the optimal solution.\n",
    "- **Too low**: Training takes too long to converge.\n",
    "- Learning rate is a **hyperparameter** that requires tuning.\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "- Defines the number of training samples used in each batch of each epoch.\n",
    "\n",
    "### Effects of Batch Size\n",
    "\n",
    "- **Smaller batch sizes**:\n",
    "  - Help escape local minima.\n",
    "  - Can make training appear inconsistent across runs due to random shuffling.\n",
    "- **Larger batch sizes**:\n",
    "  - Can get stuck in a suboptimal solution.\n",
    "\n",
    "### Recap (Important!)\n",
    "\n",
    "- **Small batch sizes** help avoid local minima.\n",
    "- **Large batch sizes** risk converging on the wrong solution.\n",
    "- **Large learning rates** may overshoot the correct solution.\n",
    "- **Small learning rates** slow down training.\n",
    "\n",
    "---\n",
    "\n",
    "_Source: Sundog Education, DataCumulus (¬© 2022)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Leanring\n",
    "Also know as Fine-Tuning Pre-Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is a powerful technique where a pre-trained model is used as a starting point for a new task, saving time and computational resources. Well known Model Zoo or model collection is called Hugging Face. This is a platform that allows users access to numerous open sourced machine learning models. \n",
    "\n",
    "---\n",
    "\n",
    "#### **Approaches to Transfer Learning**\n",
    "##### **1Ô∏è‚É£ Fine-Tuning a Pre-Trained Model**\n",
    "‚úÖ Continue training a **pre-trained model** on new data.  \n",
    "‚úÖ Ideal when the model has been trained on **far more data** than you have.  \n",
    "‚úÖ **Use a low learning rate** to **incrementally** improve performance.  \n",
    "‚úÖ **Freeze lower layers** and add **new trainable layers** to adapt to new tasks.  \n",
    "‚úÖ The model **learns to repurpose old features** for new predictions.  \n",
    "‚úÖ Hybrid approach: **First freeze layers ‚Üí Then fine-tune everything**.\n",
    "\n",
    "---\n",
    "\n",
    "##### **2Ô∏è‚É£ Retraining from Scratch**\n",
    "‚úÖ **Only do this if you have LOTS of training data**.  \n",
    "‚úÖ Data must be **very different** from the original pre-trained dataset.  \n",
    "‚úÖ Requires **high computing power** (e.g., TPUs, GPUs).  \n",
    "‚úÖ Used in cases where pre-trained knowledge **isn‚Äôt relevant**.\n",
    "\n",
    "---\n",
    "\n",
    "##### **3Ô∏è‚É£ Using a Pre-Trained Model \"As-Is\"**\n",
    "‚úÖ If the model was trained on **exactly the data you need**, no extra training is required.  \n",
    "‚úÖ Example: Using a pre-trained **ResNet** for generic **image classification**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üöÄ Choosing the Right Transfer Learning Strategy**\n",
    "| **Scenario** | **Best Approach** |\n",
    "|-------------|------------------|\n",
    "| Small dataset, similar to pre-trained model | ‚úÖ Fine-tune existing model |\n",
    "| Small dataset, very different from pre-trained model | ‚úÖ Add new layers, then fine-tune |\n",
    "| Large dataset, different from pre-trained model | üî• Retrain from scratch |\n",
    "| Model already fits your needs | ‚úÖ Use pre-trained model as-is |\n",
    "\n",
    "---\n",
    "\n",
    "### **üí° TL;DR**\n",
    "- **Fine-tuning**: Best for leveraging pre-trained knowledge while adapting to new tasks.  \n",
    "- **Retrain from scratch**: Only if you have **tons of data** and **high computational power**.  \n",
    "- **Use as-is**: If the model already fits your requirements.  \n",
    "\n",
    "üöÄ **Transfer learning saves time, improves performance, and reduces training costs!**  \n",
    "Would you like an example **implementation in TensorFlow or PyTorch?** üòä\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Regularization Techniques\n",
    "Regularization is a technique used to prevent overfitting, which occurs when a model performs well on training data but struggles with new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    "- **Definition**: Overfitting occurs when a model learns patterns that exist only in the training data rather than general trends applicable to real-world data.\n",
    "- **Key Indicator**: High accuracy on the training set but significantly lower accuracy on the test or validation set.\n",
    "- **Solution**: Regularization techniques help prevent overfitting by controlling model complexity and reducing reliance on specific patterns in training data.\n",
    "\n",
    "---\n",
    "#### **1. L1 and L2 Regularization**\n",
    "| Regularization Type | Effect on Model |\n",
    "|---------------------|----------------|\n",
    "| **L1 Regularization (Lasso)** | Eliminates some features by setting certain weights to **zero** (creates a sparse model, useful for feature selection). |\n",
    "| **L2 Regularization (Ridge)** | Shrinks all weights toward **smaller values** but **does not set them to zero** (reduces model complexity without eliminating features). |\n",
    "\n",
    "#### **2. Dropout Layer**\n",
    "- A **random subset** of neurons is dropped **during each training step**.\n",
    "- Prevents the network from relying too much on any single neuron.\n",
    "- Encourages more **distributed learning** across all neurons.\n",
    "- Helps **reduce overfitting** and improves model generalization.\n",
    "\n",
    "#### **3. Early Stopping**\n",
    "- Monitors the validation loss during training.\n",
    "- Stops training **when the validation loss starts increasing**, which signals overfitting.\n",
    "- Prevents excessive training that leads to memorization of noise in training data.\n",
    "\n",
    "#### **4. Model Simplification**\n",
    "- Overfitting in deep networks might be due to **too many layers/neurons**.\n",
    "- **Solution**: \n",
    "  - Reduce the number of layers or neurons.\n",
    "  - Use **pruning techniques** to remove unnecessary connections.\n",
    "\n",
    "---\n",
    "#### **What is the Vanishing Gradient Problem?**\n",
    "- Occurs when **gradients become extremely small** as they propagate back through deep networks.\n",
    "- Causes **very slow learning** or **stagnation** in earlier layers.\n",
    "- Common in deep **Recurrent Neural Networks (RNNs)** and deep **feedforward networks**.\n",
    "- **Opposite issue**: **Exploding Gradient Problem**, where gradients become too large.\n",
    "\n",
    "#### **How to Fix the Vanishing Gradient Problem?**\n",
    "1. **Use a better activation function**\n",
    "   - Sigmoid and Tanh suffer from vanishing gradients.\n",
    "   - **ReLU (Rectified Linear Unit)** is a better choice.\n",
    "2. **Residual Networks (ResNet)**\n",
    "   - Introduce \"skip connections\" to allow gradients to flow more easily.\n",
    "3. **Long Short-Term Memory (LSTM) for RNNs**\n",
    "   - Helps retain gradients over long sequences.\n",
    "4. **Break large networks into smaller subnetworks**\n",
    "   - Train them individually and then combine.\n",
    "5. **Gradient Clipping**\n",
    "   - Prevents gradients from becoming too large (helps with exploding gradients).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Key Takeaways for Regularization\n",
    "- **Regularization (L1 & L2)** prevents overfitting by controlling model complexity.\n",
    "- **Dropout** improves generalization by forcing the network to distribute learning.\n",
    "- **Early stopping** ensures training halts before the model starts memorizing noise.\n",
    "- **Vanishing gradients** slow down training, but can be fixed using **ReLU, ResNets, LSTMs**, and other advanced techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "This is a grid that stores the True-Positive/Negative & False-Positive/Negative outcomes of your classification model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Confusion Matrix Structure**  \n",
    "|                | **Predicted Positive** | **Predicted Negative** |\n",
    "|--------------|----------------|----------------|\n",
    "| **Actual Positive (P)** | True Positive (TP) ‚úÖ | False Negative (FN) ‚ùå |\n",
    "| **Actual Negative (N)** | False Positive (FP) ‚ùå | True Negative (TN) ‚úÖ |\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Key Performance Metrics**\n",
    "| **Metric** | **Formula** | **Interpretation** |\n",
    "|------------|------------|------------------|\n",
    "| **Accuracy** | (TP + TN) / (TP + TN + FP + FN) | Overall correctness (good for balanced data) |\n",
    "| **Precision** | TP / (TP + FP) | How often a predicted positive is correct (low FP preferred) |\n",
    "| **Recall (Sensitivity, TPR)** | TP / (TP + FN) | How many actual positives were correctly identified (low FN preferred) |\n",
    "| **F1-Score** | 2 √ó (Precision √ó Recall) / (Precision + Recall) | Harmonic mean of Precision & Recall (balance metric) |\n",
    "| **Specificity (TNR)** | TN / (TN + FP) | How well the model identifies actual negatives |\n",
    "| **False Positive Rate (FPR)** | FP / (FP + TN) | Probability of false alarms (1 - Specificity) |\n",
    "| **False Negative Rate (FNR)** | FN / (FN + TP) | Probability of missing actual positives (1 - Recall) |\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Choosing the Right Metric**\n",
    "| **Scenario** | **Best Metric to Use** |\n",
    "|-------------|----------------------|\n",
    "| **Balanced classes** | Accuracy |\n",
    "| **Minimize False Positives (e.g., spam detection, fraud detection)** | Precision |\n",
    "| **Minimize False Negatives (e.g., disease diagnosis, security threats)** | Recall (Sensitivity) |\n",
    "| **Imbalanced data** | F1-Score, AUC-ROC, Precision-Recall Curve |\n",
    "| **Detecting rare events (fraud, anomaly detection)** | Specificity |\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Model Optimization Tips**\n",
    "- **Increase Recall** (Reduce FN): Lower decision threshold, oversample minority class.\n",
    "- **Increase Precision** (Reduce FP): Increase decision threshold, feature selection.\n",
    "- **Handle Imbalanced Data**: Use SMOTE, class weighting, balanced datasets.\n",
    "- **Evaluate Model**: Use AUC-ROC for balanced data, Precision-Recall Curve for imbalanced data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are different ways to structure the **confusion matrix** while maintaining the same logical relationships:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Standard Structure (Actual vs. Predicted)**\n",
    "|                | **Predicted Positive** | **Predicted Negative** |\n",
    "|--------------|----------------|----------------|\n",
    "| **Actual Positive (P)** | True Positive (**TP**) ‚úÖ | False Negative (**FN**) ‚ùå |\n",
    "| **Actual Negative (N)** | False Positive (**FP**) ‚ùå | True Negative (**TN**) ‚úÖ |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Flipped Rows (Negative First)**\n",
    "|                | **Predicted Positive** | **Predicted Negative** |\n",
    "|--------------|----------------|----------------|\n",
    "| **Actual Negative (N)** | False Positive (**FP**) ‚ùå | True Negative (**TN**) ‚úÖ |\n",
    "| **Actual Positive (P)** | True Positive (**TP**) ‚úÖ | False Negative (**FN**) ‚ùå |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Flipped Columns (Negative First)**\n",
    "|                | **Predicted Negative** | **Predicted Positive** |\n",
    "|--------------|----------------|----------------|\n",
    "| **Actual Positive (P)** | False Negative (**FN**) ‚ùå | True Positive (**TP**) ‚úÖ |\n",
    "| **Actual Negative (N)** | True Negative (**TN**) ‚úÖ | False Positive (**FP**) ‚ùå |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Fully Flipped (Negative First in Both Rows & Columns)**\n",
    "|                | **Predicted Negative** | **Predicted Positive** |\n",
    "|--------------|----------------|----------------|\n",
    "| **Actual Negative (N)** | True Negative (**TN**) ‚úÖ | False Positive (**FP**) ‚ùå |\n",
    "| **Actual Positive (P)** | False Negative (**FN**) ‚ùå | True Positive (**TP**) ‚úÖ |\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Labeling with Abbreviations**\n",
    "|                | **Pred +** | **Pred -** |\n",
    "|--------------|---------|---------|\n",
    "| **Actual +** | TP ‚úÖ | FN ‚ùå |\n",
    "| **Actual -** | FP ‚ùå | TN ‚úÖ |\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Labeling with Full Descriptions**\n",
    "|                | **Predicted as Positive** | **Predicted as Negative** |\n",
    "|--------------|------------------|------------------|\n",
    "| **Actually Positive** | True Positive (**TP**) ‚úÖ | False Negative (**FN**) ‚ùå |\n",
    "| **Actually Negative** | False Positive (**FP**) ‚ùå | True Negative (**TN**) ‚úÖ |\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Using Symbols Instead of Text**\n",
    "|                | **Predicted ‚úÖ** | **Predicted ‚ùå** |\n",
    "|--------------|----------------|----------------|\n",
    "| **Actual ‚úÖ** | ‚úÖ TP | ‚ùå FN |\n",
    "| **Actual ‚ùå** | ‚ùå FP | ‚úÖ TN |\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Numerical Example Format**\n",
    "|                | **Predicted Positive** | **Predicted Negative** |\n",
    "|--------------|----------------|----------------|\n",
    "| **Actual Positive (100 cases)** | TP = 80 | FN = 20 |\n",
    "| **Actual Negative (200 cases)** | FP = 30 | TN = 170 |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best to use Record-IO/Protobuf data format from s3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Learner\n",
    "- linear regression, and classification if data is linear. \n",
    "- has pipe/file mode file mode carries over each file individually and in pipe mode it sends it all in datapipline \n",
    "- Data should be scaled and shuffled\n",
    "- good for mulitple cpu or 1 gpu not multiple \n",
    "\n",
    "XGboost\n",
    "- Boosted group of decision trees\n",
    "- Uses Gradient descent \n",
    "- computational effective, can be used for regression and classification \n",
    "\n",
    "seq2seq\n",
    "- Used in translation, summarization, speech recognition, etc.\n",
    "- Needs data in tokenized form and a vocab list(List of what each token represents) Ex(1, 'Helllo') 1 = token 'Hello' = word \n",
    "\n",
    "DeepAR\n",
    "- RNN, forecast timeseries data. stock prices\n",
    "- can train multiple timeseries parrallely \n",
    "\n",
    "BlazingText\n",
    "- text classification, predict labels for sentences \n",
    "- supervised model, just used for sentences not large text documents\n",
    "- has Word2Vec, embedding words that are similar together, just works on words. \n",
    " \n",
    "object2vec"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
