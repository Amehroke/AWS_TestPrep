{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\">Data Engineering</h1>\n",
    "<h3 align=\"center\">Moving, Storing and Processing data in AWS</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To start off we will talk about the storage services avalibale on AWS.\n",
    "\n",
    "1. **Amazon S3 (Simple Storage Service):** Object storage for unstructured data and data lakes.\n",
    "2. **Amazon Redshift:** Fully managed data warehouse designed for large-scale analytics.\n",
    "3. **Amazon RDS/Aurora**: Managed relational databases (not explicitly listed but implied as commonly used in AWS architectures).\n",
    "4. **Amazon DynamoDB:** NoSQL key-value and document database for high-performance applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 (Simple Storage Service - DataLake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used for storage lakes for unstructured data, (Data Lake). \n",
    "\n",
    "It is used by method called buckets in AWS \n",
    "- Example:  Image_Data/Animals/Dog/Pitbull\n",
    "    - In this each label is a bucket and they are globaly unique so in your entire AWS account only one Bucket can have that name. \n",
    "    - This full path of the object is refered to as the key\n",
    "    - Image_Data, Animals, Dog, Pitbull are seperate buckets\n",
    "    - Object storage => supports any file format\n",
    "**Backbone for many AWS ML services (example: SageMaker)** \n",
    "\n",
    "Limitations: \n",
    "- Max object size is 5TB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage Classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Amazon S3 Standard - General Purpose\n",
    "- Amazon S3 Standard-Infrequent Access (IA)\n",
    "- Amazon S3 One Zone-Infrequent Access\n",
    "- Amazon S3 Glacier Instant Retrieval\n",
    "- Amazon S3 Glacier Flexible Retrieval\n",
    "- Amazon S3 Glacier Deep Archive\n",
    "- Amazon S3 Intelligent Tiering\n",
    "- Can move between classes manually or using S3 Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Storage Class**                          | **Use Cases**                                | **Retrieval Times**             | **Cost**                             |\n",
    "|--------------------------------------------|---------------------------------------------|---------------------------------|--------------------------------------|\n",
    "| **Amazon S3 Standard - General Purpose**   | Frequently accessed data.                   | Immediate                      | Highest cost per GB.                |\n",
    "| **Amazon S3 Standard-Infrequent Access (IA)** | Long-lived, infrequently accessed data. But requires immediate acess when needed. **Backups**     | Immediate                      | Lower storage cost, retrieval fees. |\n",
    "| **Amazon S3 One Zone-Infrequent Access**   | Secondary backups, easily reproducible data. | Immediate                      | Cheaper than Standard-IA.           |\n",
    "| **Amazon S3 Glacier Instant Retrieval**    | Archival data requiring millisecond access.  | Milliseconds                   | Low storage cost, higher retrieval fees. |\n",
    "| **Amazon S3 Glacier Flexible Retrieval**   | Long-term archival with occasional access.   | 1–5 minutes (expedited),3–5 hours (standard), 5–12 hours (bulk).       | Cheaper than Instant Retrieval.      |\n",
    "| **Amazon S3 Glacier Deep Archive**         | Regulatory or long-term archival storage.    | 12–48 hours                    | Lowest storage cost.                 |\n",
    "| **Amazon S3 Intelligent-Tiering**          | Data with unpredictable or dynamic access patterns. **Can move between classes manually or using S3 Lifecycle** | Varies by access tier               | Monitoring fee, cost varies by access tier. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifecycles of S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This displays the lifecycles for infrequent data:\n",
    "\n",
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <!-- Text on the left -->\n",
    "    <div style=\"flex: 1; padding-right: 20px;\">\n",
    "        <p>We can set up actions to move tiers and delete objects:</p>\n",
    "        <ul>\n",
    "            <li>Move objects to Standard IA class 60 days after creation</li>\n",
    "            <li>Move to Glacier for archiving after 6 months</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"../Figures/Data_Engineering/S3_Lifecycles.png\" alt=\"S3 Lifecycles\" style=\"width: 400px;\">\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Example Problem:**\n",
    "- Your application on EC2 creates images thumbnails after profile photos are uploaded to Amazon S3. These thumbnails can be easily recreated, and only need to be kept for 60 days. The source images should be able to be immediately retrieved for these 60 days, and afterwards, the user can wait up to 6 hours. How would you design this?\n",
    "\n",
    "    - S3 source images can be on Standard, with a lifecycle configuration to transition them to Glacier after 60 days\n",
    "    - S3 thumbnails can be on One-Zone IA, with a lifecycle configuration to expire them (delete them) after 60 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <!-- Text on the left -->\n",
    "    <div style=\"flex: 1; padding-right: 20px;\">\n",
    "        <h3>Amazon S3 Analytics – Storage Class Analysis</h3>\n",
    "        <ul>\n",
    "            <li>Helps you decide when to transition objects to the right storage class.</li>\n",
    "            <li>It only recommends for Standard and Standard IA, <strong>Does NOT work for One-Zone IA or Glacier.</strong></li>\n",
    "            <li>Report is updated daily, and it takes 24 to 48 hours to start seeing data analysis.</li>\n",
    "            <li><strong>Main purpose is to give insights on S3 lifecycles.</strong></li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"../Figures/Data_Engineering/S3_Analytics.png\" alt=\"Placeholder for S3 Analytics Image\" style=\"width: 200px; border: 1px dashed #ccc;\">\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 security can be managed using **IAM roles** and **bucket policies**, which can be used either **together** or **independently**.\n",
    "\n",
    "- For public access(WorldWideWeb) we use the bucket policies. \n",
    "    - The **Block Public Access** setting prevents a bucket from being public, overriding any bucket policy, to mitigate data leaks.\n",
    "\n",
    "- For example:\n",
    "    - You attach an IAM role to an EC2 instance to allow it to access S3.\n",
    "    - The S3 bucket policy ensures only that specific IAM role can perform actions on a specific bucket.\n",
    "\n",
    "|IAM Role\t|Bucket Policy|\n",
    "|---|---|\n",
    "|Managed at the IAM level, tied to principals.|\tManaged at the S3 bucket level.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encryption of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4 Methods for Encrypting Objects in S3 Buckets**\n",
    "\n",
    "**Server-Side Encryption (SSE)**  \n",
    "   Encryption is handled by Amazon S3 on the server side.\n",
    "\n",
    "   1. **SSE with Amazon S3-Managed Keys (SSE-S3)** - **Default Option**  \n",
    "     - Encrypts S3 objects using keys managed and owned by AWS. \n",
    "     - Encryption is done serverside. \n",
    "\n",
    "   2. **SSE with KMS Keys (SSE-KMS)**  \n",
    "     - Leverages **AWS Key Management Service (KMS)** for key management.  \n",
    "     - Ideal for tighter control over key usage and permissions.\n",
    "\n",
    "   3. **SSE with Customer-Provided Keys (SSE-C)**  \n",
    "     - Allows you to manage your own encryption keys.  \n",
    "     - Keys must be provided with every request to Amazon S3.\n",
    "\n",
    "**Client-Side Encryption**  \n",
    "   4. Encryption occurs **before uploading objects to S3**.  \n",
    "   - Clients are responsible for managing encryption keys and operations.\n",
    "\n",
    "#### **Key Exam Tip**  \n",
    "Understand which encryption method applies to different scenarios for the exam, focusing on **SSE-KMS** for fine-grained control and **SSE-S3** for default managed encryption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. SSE with Amazon S3-Managed Keys (SSE-S3)  \n",
    "Data is encrypted within AWS and stored in an S3 bucket, with encryption keys fully managed by AWS (not accessible to the owner).\n",
    "\n",
    "<div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"../Figures/Data_Engineering/S3_Encryption_1.png\" alt=\"S3 Lifecycles\" style=\"width: 400px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. SSE with KMS Keys (SSE-KMS)  \n",
    "- Data is encrypted using customer-managed keys via AWS KMS, allowing S3 owners to control access and track key usage with CloudTrail. \n",
    "- Users need permissions in both the bucket policy and KMS key policy to access and decrypt data.\n",
    "\n",
    "\n",
    "<div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"../Figures/Data_Engineering/S3_Encryption_2.png\" alt=\"S3 Lifecycles\" style=\"width: 400px;\">\n",
    "</div>\n",
    "\n",
    "**Limitations:**\n",
    "- Increase Latency in workflows. \n",
    "- API Limits: KMS API calls (e.g., Encrypt, Decrypt, GenerateDataKey) are subject to request rate limits, which could cause throttling if exceeded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. SSE with Customer-Provided Keys (SSE-C)\n",
    "The encryption key is generated and managed on the client side. Data is sent with key and then AWS encriypts and discards key to store into S3. To read the client will provide the key again to decrypt the data. **Main: Key is client side and not stored in AWS**\n",
    "\n",
    "<div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"../Figures/Data_Engineering/S3_Encryption_3.png\" alt=\"S3 Lifecycles\" style=\"width: 400px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Client-Side Encryption  \n",
    "Data is encrypted before being uploaded to S3, with the owner managing the entire encryption lifecycle. Both encryption and decryption occur outside of AWS.\n",
    "\n",
    "<div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"../Figures/Data_Engineering/S3_Encryption_4.png\" alt=\"S3 Lifecycles\" style=\"width: 400px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encryption in Transit  \n",
    "AWS ensures data in transit is encrypted using SSL/TLS for API endpoints. HTTPS is recommended for all communications and is **mandatory for SSE-C** #3. To enforce HTTPS for other encryption methods, you can configure bucket policies to block HTTP connections and allow only HTTPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS RedShift (Data Warehouse for Analytics 📊-SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Data Warehouse, that uses SQL analytics (OLAP - Online analytical processing)\n",
    "- Loads data from S3 to Redshift\n",
    "- Use Redshift Spectrum to query data directly in S3 (no loading)\n",
    "- Data is stored in columns\n",
    "\n",
    "✅ Use Case: Business intelligence (BI), analytics, machine learning workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon RDS Aurora (Managed Relational Database 🏦-SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Relational Store, SQL (OLTP - Online Transaction Processing)\n",
    "- Data is stored in rows, used for transactions\n",
    "\n",
    "- ✅ Best for: Transactional (OLTP) workloads with high availability & performance.\n",
    "- ✅ Type: Relational database (fully managed PostgreSQL/MySQL-compatible).\n",
    "- ✅ Use Case: Web apps, e-commerce, enterprise applications, SaaS.\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon DynamoDB – (Key-Value Database ⚡ - NoSQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NoSQL data store, serverless, provision read/write capacity\n",
    "\n",
    "**Useful to store a machine learning model metrics and infrence results, NOT ACTUAL MODEL**\n",
    "\n",
    "- ✅ Best for: High-performance, low-latency NoSQL workloads.\n",
    "- ✅ Type: NoSQL key-value & document store (fully managed, serverless).\n",
    "- ✅ Use Case: Real-time apps, IoT, gaming, caching, leaderboards, recommendation engines.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Database Migration Service (DMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Database Migration Service (DMS) is a fully managed service that helps migrate databases and data warehouses to AWS with minimal downtime.\n",
    "\n",
    "**Example Use Case:** \n",
    "- Transactional data is stored in an on-premises Oracle database, but SageMaker requires data in S3 for training. AWS DMS is used to continuously replicate data from Oracle to Amazon S3.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinesis Data Streams (Server-Less)(Real-Time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Kinesis Data Streams is a real-time data streaming service that enables you to capture, process, and analyze continuous data streams at scale. It’s often used for applications requiring real-time processing, such as log processing, IOTs, or machine learning pipelines.\n",
    "\n",
    "<div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"../Figures/Data_Engineering/Kinesis_DataStream.png\" alt=\"S3 Lifecycles\" style=\"width: 600px;\">\n",
    "        <p>\n",
    "                Producers are the agents/appliations/code etc. that connects the realtime data stream to Kinesis. Kinesis then forwards the data to the consumers in real time.  \n",
    "        </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Details: \n",
    "- Streaming data collection\n",
    "- Producer & Consumer code, Needs Code. \n",
    "- Real-time\n",
    "- Provisioned / On-Demand mode\n",
    "- Data storage up to 365 days\n",
    "- Replay Capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinesis Video Stream (Server-less)(Real-time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"../Figures/Data_Engineering/KinesisVideoWorkflow.png\" alt=\"Kinesis Data Flow\" style=\"width: 600px;\">\n",
    "    </div>\n",
    "\n",
    "\n",
    "### Kinesis Video Streams Workflow\n",
    "\n",
    "1. **Ingest Video**: Real-time video is streamed via **Kinesis Video Streams** from a producer (e.g., IoT cameras).  \n",
    "2. **Processing App**: A consumer application in a Docker container (e.g., on EC2) processes the stream, checkpointing progress to **DynamoDB** for recovery.  \n",
    "3. **ML Inference**: Decoded frames are sent to **Amazon SageMaker** for machine learning predictions (e.g., object detection).  \n",
    "4. **Stream Results**: Inference results are published to a **Kinesis Data Stream**.  \n",
    "5. **Real-Time Actions**: **Lambda functions** consume the data stream to trigger notifications or other actions.\n",
    "\n",
    "**Use Case**: Real-time detection of anomalies, like identifying a burglar in a house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Kinesis Video Stream Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "    <div style=\"flex: 1; padding-right: 20px;\">\n",
    "        <p>\n",
    "            Amazon Kinesis Video Streams is a managed service designed to stream, process, and store video and audio data in real-time. It is specifically tailored for applications that deal with video feeds from connected devices (like IoT cameras), audio streams, or other time-encoded data (e.g., radar signals).\n",
    "        </p>\n",
    "        <ul>\n",
    "            <li><strong>Producers:</strong>\n",
    "                <ul>\n",
    "                    <li>Security camera, body-worn camera, AWS DeepLens, smartphone camera</li>\n",
    "                    <li>Audio feeds, images, RADAR data, RTSP camera</li>\n",
    "                    <li>One producer per video stream</li>\n",
    "                    <li>Video playback capability</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li><strong>Consumers:</strong>\n",
    "                <ul>\n",
    "                    <li>Build your own (MXNet, TensorFlow)</li>\n",
    "                    <li>AWS SageMaker</li>\n",
    "                    <li>Amazon Rekognition Video</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Data retention: Keep data for 1 hour to 10 years</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"../Figures/Data_Engineering/KinesisVideoStream.png\" alt=\"Kinesis Data Flow\" style=\"width: 600px;\">\n",
    "        <p style=\"margin-top: 10px; font-style: italic;\">Illustration of data producers and consumers in Kinesis Video Streams</p>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Data Firehouse (Server-Less)(Near Real-Time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Kinesis Data Firehose is a consumer of incoming data that routes it to the desired destination. It also supports integration with AWS Lambda for data transformation during the streaming process.\n",
    "\n",
    "<div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"../Figures/Data_Engineering/DataFirhouse.png\" alt=\"S3 Lifecycles\" style=\"width: 600px;\">\n",
    "        <p>\n",
    "            Can send data to destinations <strong>Near-Real time</strong> with buffering, it is <strong>Server-Less</strong>\n",
    "        </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Details: \n",
    "- Load streaming data into S3 / Redshift / OpenSearch / 3rd party /custom HTTP\n",
    "- Fully managed, No code needed.\n",
    "- Near real-time\n",
    "- Automatic scaling\n",
    "- No data storage\n",
    "- Doesn’t support replay capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Managed Service for Apache Flink (Server-Less)(Real-Time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also known as Kinesis Data Analytics\n",
    "- It’s used for complex event processing, aggregations, joins, and anomaly detection in real-time streaming data.\n",
    "- Streaming ETL\n",
    "- Continuous metric generation\n",
    "- Responsive real time analytics\n",
    "\n",
    "------ **Does not work with data fire-hose** ----- **Only with Kinesis Data Stream real time** ------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Glue (serverless)(Some Services real-time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AWS Glue** is a fully managed **data integration service** that enables **ETL (Extract, Transform, Load)** operations across various AWS and external data sources. It consists of multiple components that work together for **data cataloging, transformation, and real-time streaming.**\n",
    "\n",
    "### **Key Components of AWS Glue**\n",
    "| **Component**             | **Purpose** |\n",
    "|---------------------------|------------|\n",
    "| **Glue Data Catalog**     | Metadata repository for schema and table definitions. Works with Athena, Redshift, etc. |\n",
    "| **Glue Crawlers**         | Automatically scan and classify data, adding schema details to the Glue Data Catalog. (Not real time)|\n",
    "| **Glue ETL Jobs**         | Serverless data transformation using **Apache Spark** or **Python (PySpark/Scala)**. |\n",
    "| **Glue Studio**           | No-code **visual ETL editor** for designing and running ETL workflows. |\n",
    "| **Glue DataBrew**         | No-code tool for **data cleaning and transformation**. Ideal for business analysts. |\n",
    "| **Glue Streaming ETL**    | **Processes real-time streaming data** from **Kinesis, Kafka, etc.** |\n",
    "| **Glue ML Transforms**    | **Machine learning-based** data deduplication and cleansing. |\n",
    "\n",
    "### **Summary**\n",
    "AWS Glue is a **unified service** with multiple capabilities, providing **serverless data processing, transformation, and cataloging** for analytics and machine learning.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Athena (Serverless)(Near-RealTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "AWS Athena is a serverless, interactive query service that allows you to run SQL queries on data stored in Amazon S3 without needing to set up or manage databases or servers.\n",
    "\n",
    "How AWS Athena Works:\n",
    "\n",
    "1. Data is stored in Amazon S3 (raw or processed).\n",
    "2. AWS Glue Crawlers scan and create schema metadata in the AWS Glue Data Catalog.\n",
    "3. Athena runs SQL queries directly on S3 without needing a database.\n",
    "4. Results are stored in S3 and can be used for analytics, dashboards, or ML processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Batch (Serverless)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Batch – Fully Managed Batch Processing Service\n",
    "\n",
    "**For any non-ETL related work, Batch is probably better**\n",
    "\n",
    "**For ETL related jobs Glue is better**\n",
    "\n",
    "####  **Is likely used for Cost-Efficent ML training outside of Sagemaker, aids to reduce costs but requires manual compute allocation.**\n",
    "\n",
    "- ✅ Best for: Running large-scale batch processing jobs efficiently on AWS.\n",
    "- ✅ Type: Fully managed batch job scheduler & compute orchestrator.\n",
    "- ✅ Use Cases: ML training, data processing, financial modeling, genomics.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Step-Functions(Not-RealTime)(Serverless)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(AWS services Automation of  in a workflow)**\n",
    "\n",
    "AWS Step Functions is a serverless orchestration service that allows you to define, execute, and manage workflow sequences across multiple AWS services. It ensures fault tolerance, state management, and parallel execution without needing custom scripts.\n",
    "\n",
    "- ✅ Best for: Orchestrating and automating workflows across multiple AWS services.\n",
    "- ✅ Type: Serverless workflow automation service.\n",
    "- ✅ Use Cases: ML pipelines, ETL orchestration, data processing, event-driven workflows.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1 align=\"center\">Real-Time ML Workflows Examples</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1️⃣ Real-Time Fraud Detection (Streaming ML Inference)\n",
    "\n",
    "### 💡 Use Case: Detect fraudulent transactions in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Real-Time Fraud Detection Workflow</h2>\n",
    "<img src=\"../Figures/Data_Engineering/MLWorkFlows/FraudMLWorkFlow.png\" style=\"width:800px; display:block; margin:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FYI Their is multiple ways to make this work flow, for instance we can use Data Firehouse for the main ingestion from the procucer, however this will not be real-time but near-real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2️⃣ Real-Time Video Classification\n",
    "\n",
    "### 💡 Use Case: Perform Computer Vision on video in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Real-Time Video Classification-Rekognition</h2>\n",
    "<img src=\"../Figures/Data_Engineering/MLWorkFlows/VideoStream1.png\" style=\"width:800px; display:block; margin:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Real-Time Video Classification-Sagemaker</h2>\n",
    "<img src=\"../Figures/Data_Engineering/MLWorkFlows/VideoStream2.png\" style=\"width:800px; display:block; margin:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Migration Workflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Figures/Data_Engineering/MLWorkFlows/DataMigrationWF.png\" style=\"width:800px; display:block; margin:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AWS Glue Data Catalog and its crawlers help track data schemas throughout the migration pipeline, ensuring data is updated correctly and identifying any unintended changes in the workflow.**\n",
    "\n",
    "**AWS Batch is a fully managed service that runs large-scale batch computing workloads, such as data processing or script execution, on demand or on a scheduled basis using EventBridge.**\n",
    "\n",
    "**In this workflow AWS Batch is used to create jobs that processes data storage by periodically cleaning or executing large-scale scripts based on a scheduled time basis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To **orchestrate** all these workflows, we use **AWS Step Functions** to connect services, manage endpoints, and ensure a **fault-tolerant ML workflow environment**.\n",
    "<img src=\"../Figures/Data_Engineering/MLWorkFlows/Stepfunctions.png\" style=\"width:100px; display:block; margin:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **📌 Data Engineering Summary**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **🔹 Data Storage & Access**  \n",
    "- **Amazon S3** → Object storage for your data.  \n",
    "- **VPC Endpoint Gateway** → Privately access your S3 bucket without using the public internet.  \n",
    "\n",
    "---\n",
    "\n",
    "### **🔹 Real-Time & Streaming Data Processing**  \n",
    "- **Kinesis Data Streams** → Real-time data streaming for applications; requires capacity planning.  \n",
    "- **Kinesis Data Firehose** → Near real-time data ingestion to **S3, Redshift, Elasticsearch, Splunk**.  \n",
    "- **Kinesis Data Analytics** → Perform **SQL transformations on streaming data**.  \n",
    "- **Kinesis Video Streams** → Real-time video ingestion and processing.  \n",
    "\n",
    "---\n",
    "\n",
    "### **🔹 Data Cataloging & ETL**  \n",
    "- **Glue Data Catalog & Crawlers** → Metadata repository for schemas and datasets.  \n",
    "- **Glue ETL** → Serverless **ETL jobs using Apache Spark**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **🔹 Databases & Warehouses**  \n",
    "- **DynamoDB** → **NoSQL** key-value store for scalable applications.  \n",
    "- **Redshift** → Data warehousing for **OLAP (Online Analytical Processing)** with **SQL support**.  \n",
    "- **Redshift Spectrum** → Query **S3 data using Redshift** without loading it into Redshift.  \n",
    "- **RDS / Aurora** → **Relational database (OLTP)** supporting SQL.  \n",
    "---\n",
    "\n",
    "### **🔹 Data Orchestration & Processing**  \n",
    "- **Data Pipelines** → Orchestrate **ETL jobs** across **RDS, DynamoDB, and S3** (runs on EC2).  \n",
    "- **AWS Batch** → **Batch processing using Docker containers**; manages EC2 instances for you.  \n",
    "- **DMS (Database Migration Service)** → **1-to-1 database replication** (Change Data Capture - CDC), not ETL.  \n",
    "- **Step Functions** → **Orchestrate workflows** with **audit trails & retry mechanisms**.  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
