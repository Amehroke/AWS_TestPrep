{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\">Exploratory Data Analysis</h1>\n",
    "<h3 align=\"center\">Analyzing and visualizing data in AWS</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Reqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To start off what needs to be known for EDA\n",
    "\n",
    "1. **Python** - But the test will not test your Python knowledge.\n",
    "2. **Scikit_learn** Python library for machine learning models\n",
    "\n",
    "#### Types of Data:\n",
    "- Numerical\n",
    "- Categorical\n",
    "- Ordinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. A Probability Density Function (PDF) \n",
    "\n",
    "<h4 align=\"center\">Represents the probability distribution of a continuous random variable. Gives the likelihood (density) that the variable falls within a given range.</h4>\n",
    "<img src=\"../Figures/EDA/PDF.png\" style=\"width:400px; display:block; margin:auto;\">\n",
    "\n",
    "\n",
    "#### 2. A Probability Mass Function (PMF) \n",
    "<h4 align=\"center\">Gives the exact probability of a discrete random variable taking on a specific value. NOT CONTINOUS</h4>\n",
    "<img src=\"../Figures/EDA/PMF.png\" style=\"width:400px; display:block; margin:auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Poisson Distribution** is another Example of a PMF it works with discrete values. We cant sell 1/2 of a car, so PMF is a distribution we will use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonality (repeating patterns over time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Seasonality is looking at a period in the data set that presents a repetative pattern.</h2>\n",
    "<img src=\"../Figures/EDA/Seasonality.png\" style=\"width:400px; display:block; margin:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend (long-term movement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Trends is looking at the entire data set to see a upward or downward trend.</h2>\n",
    "<p align=\"center\">In this Example the trend is upward</p>\n",
    "<img src=\"../Figures/EDA/Trends.png\" style=\"width:400px; display:block; margin:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - Sesonality = Trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">If we calculate the seasonality and subtract it from the entire dataset it will give us the smooth line trend of the data</h3>\n",
    "<p align=\"center\">In this Example we subtract the sesonality from the dataset to get the smooth trend curve</p>\n",
    "<img src=\"../Figures/EDA/TrendSeason.png\" style=\"width:400px; display:block; margin:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series data consists of **trend**, **seasonality**, and **noise**. \n",
    "\n",
    "- Use an **additive model** when seasonal effects stay constant over time (e.g., sales increase by a fixed amount each year).\n",
    "- Use a **multiplicative model** when seasonality scales with the trend (e.g., sales double during holidays as the business grows).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Athena (Serverless)(Near-RealTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "AWS Athena is a serverless, interactive query service that allows you to run SQL queries on data stored in Amazon S3 without needing to set up or manage databases or servers.\n",
    "\n",
    "**Use Case:** when you have large datasets in S3 and need quick SQL queries without managing a database.\n",
    "\n",
    " **Cost Breakdown 💰**\n",
    "- **Pay-as-you-go**: No upfront costs, pay only for queries run.\n",
    "- **$5 per TB scanned**: Charges are based on the amount of data scanned by queries.\n",
    "\n",
    " **Ways to Reduce Costs 💡**\n",
    "- **Use columnar storage formats** (e.g., **ORC, Parquet**).\n",
    "  - Improves query performance.\n",
    "- **Partition and compress data** to optimize query efficiency.\n",
    "\n",
    "- **AWS Glue** (for data cataloging) and **Amazon S3** (for data storage) have their **own separate charges**.\n",
    "\n",
    "🔹 **Tip:** Optimize your data storage strategy to minimize the data scanned and reduce costs!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS QuickSight (Serverless)(Visualization of Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An application that helps generate visualizations and business insights from large datasets efficiently.\n",
    "\n",
    "Machine Learning Capabilites:\n",
    "- Anomaly detection\n",
    "- Forecasting\n",
    "- Auto-narratives\n",
    "\n",
    "### QuickSight Visual Types\n",
    "\n",
    "AutoGraph: based on the properties on the data gentrates a graph associated to the data, but is not always right. \n",
    "\n",
    "Bar Charts\n",
    "- For comparison and distribution (histograms)\n",
    "\n",
    "Line graphs\n",
    "- For changes over time\n",
    "\n",
    "Scatter plots, heat maps\n",
    "- For correlation\n",
    "\n",
    "Pie graphs, tree maps\n",
    "- For aggregation\n",
    "\n",
    "Pivot tables\n",
    "- For tabular data\n",
    "\n",
    "### **Know which charts are for which dataset, might be a question on the test**\n",
    "### Example: Based on this data what is the best visualization for it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Elastic Map Reduce (EMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazon EMR (Elastic MapReduce)** is a managed big data platform that allows you to run Apache Spark, Hadoop, Presto, Hive, and other big data frameworks at scale on AWS. It is designed for processing large-scale datasets across a distributed computing environment.\n",
    "\n",
    "It is designed for processing **massive datasets**, making it easier to **prepare, normalize, and scale** data before feeding it into machine learning algorithms. \n",
    "\n",
    "### 💡 Spark is usually preferred over Hadoop for new projects because it's faster and supports real-time processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMR Cluster Architecture & Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align=\"center\">Represents the default Architecture of EMR, core and task nodes can be added to make the Architecture bigger. Also you can run mutilple different EMR clusters for different tasks</h4>\n",
    "<img src=\"../Figures/EDA/EMRArch.png\" style=\"width:600px; display:block; margin:auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark ML Lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **What is Spark MLlib**\n",
    "A key advantage of Spark MLlib is that it enables **Common ML algorithms to run on distributed computing clusters**. In contrast, libraries like **Scikit-Learn** are designed for single-node execution and do not natively support distributed computing.\n",
    "\n",
    "#### **🚀 Machine Learning Algorithms in Spark MLlib**\n",
    "Spark MLlib provides a variety of machine learning models optimized for large-scale data processing:\n",
    "\n",
    "- **Classification**: Logistic Regression, Naïve Bayes  \n",
    "- **Regression**: Linear and Non-Linear Regression Models  \n",
    "- **Decision Trees**: Random Forest, Gradient-Boosted Trees  \n",
    "- **Recommendation Systems**: Alternating Least Squares (ALS)  \n",
    "- **Clustering**: K-Means  \n",
    "- **Topic Modeling**: Latent Dirichlet Allocation (LDA)  \n",
    "- **ML Workflow Utilities**: Pipelines, Feature Transformation, Model Persistence  \n",
    "- **Dimensionality Reduction**: Singular Value Decomposition (SVD), Principal Component Analysis (PCA)  \n",
    "- **Statistics**: Summary Statistics, Hypothesis Testing  \n",
    "\n",
    "💡 **Key Takeaway**: If you need scalable machine learning for **big data**, Spark MLlib is a better choice than traditional libraries like **Scikit-Learn**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WorkFlow for EMR Model training\n",
    "\n",
    "#### ✅ Spark on Core Nodes → Handles distributed data preprocessing & ETL across multiple machines.\n",
    "\n",
    "#### ✅ PyTorch on Task Nodes → Performs matrix multiplications & model training using available hardware (CPU or GPU).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is Feature Engineering?**\n",
    "Feature engineering is the process of **transforming raw data into meaningful inputs** that improve machine learning model performance. It involves **selecting, creating, and modifying features** to highlight patterns in data. Common techniques include **normalization, one-hot encoding, feature scaling, dimensionality reduction (PCA), and interaction terms**. Good feature engineering enhances model accuracy by making important patterns more detectable. 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Example:** Say we have a Person, their features could include height, age, money he makes, education etc. \n",
    "There are 100s 10000s of features we can create, but feature engineering is deciding which ones are the most important, if we dont have enough features how can we create more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Curse of Dimensionality**\n",
    "**The Curse of Dimensionality** refers to the problem where adding more features (dimensions) makes data sparser, increasing computation time and reducing model performance. When we add too many feature vectors, our data moves into a higher-dimensional space. This increases the possible solutions, making the data more sparse, meaning points are farther apart. As a result, models struggle to find meaningful patterns, leading to overfitting and poor generalization. **This is known as the Curse of Dimensionality.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Common Feature Engineering Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding\n",
    "One-hot encoding is a method used to convert categorical variables into a binary matrix. Our dataset has some columns filled with words such as gender, education, and cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " City  Education  Gender\n",
      "    0          1       0\n",
      "    1          0       0\n",
      "    0          0       1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "features = ['Education', 'City', 'Gender']\n",
    "## Perform one-hot encoding\n",
    "df_encoded = pd.get_dummies(features,dtype=int)\n",
    "\n",
    "print(df_encoded.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, **One-Hot Encoding** transforms categorical labels into **binary vectors**:  \n",
    "- **Education** → `010`  \n",
    "- **City** → `100`  \n",
    "- **Gender** → `0001`  \n",
    "\n",
    "Each category is represented as a **unique binary vector**. 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling\n",
    "Feature scaling is crucial when working with numerical features that have different scales. Scaling ensures that all features contribute equally to the model, preventing any one feature from dominating the others. Also aids with computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Example ‘Age’ and ‘Income’ might have different scales. ‘Age’ might go from 0 to 100, while ‘Income’ could go from 20,000 to 200,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   Age  Income\n",
      "0   25   50000\n",
      "1   30   60000\n",
      "2   35   75000\n",
      "3   40  100000\n",
      "4   45   90000\n",
      "\n",
      "Scaled Data:\n",
      "[[-1.41421356 -1.35581536]\n",
      " [-0.70710678 -0.81348922]\n",
      " [ 0.          0.        ]\n",
      " [ 0.70710678  1.35581536]\n",
      " [ 1.41421356  0.81348922]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data  = pd.DataFrame({'Age': [25, 30, 35, 40, 45],\n",
    "                      'Income': [50000, 60000, 75000, 100000, 90000],})\n",
    "\n",
    "Scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Print results\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nScaled Data:\")\n",
    "print(Scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the code output above, now we can see that the 'Age' and 'Income' have been scaled to have a mean of 0 and a standard deviation of 1. This makes them compatible for modeling and helps algorithms that rely on distances or gradients perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction\n",
    "Dimensionality reduction is the process of reducing the number of features while preserving important information. Techniques like **PCA (Principal Component Analysis)** transform data into fewer dimensions by finding the most **variance-preserving** directions, while **t-SNE (t-Distributed Stochastic Neighbor Embedding)** is used for **visualizing high-dimensional data** by mapping it to 2D or 3D space. This helps improve model efficiency, reduce overfitting, and handle the **Curse of Dimensionality**. 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PC1           PC2\n",
      "0  1.0  1.913261e-16\n",
      "1  3.0 -4.527166e-17\n",
      "2 -1.0 -8.030383e-17\n",
      "3 -3.0  4.527166e-17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data (5 features)\n",
    "data = np.array([[2, 8, 4, 6, 10],\n",
    "                 [1, 9, 3, 5, 8],\n",
    "                 [3, 7, 5, 7, 12],\n",
    "                 [4, 6, 6, 8, 14]])\n",
    "\n",
    "# Standardize the data (PCA works best when data is scaled)\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA to reduce dimensions to 2\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "df_pca = pd.DataFrame(data_pca, columns=['PC1', 'PC2'])\n",
    "print(df_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**  \n",
    "Your dataset primarily varies along **one dimension** (PC1), meaning it captures most of the data's structure. **PC2 contributes very little**, so reducing the dataset to **only PC1** may be sufficient.  \n",
    "\n",
    "**🔥 Why Keep Only PC1?**  \n",
    "✔ **Simplifies the data** 🚀  \n",
    "✔ **Speeds up ML model training** 🏋️‍♂️  \n",
    "✔ **Eliminates unnecessary/noisy dimensions** 🎯  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to do with Missing data\n",
    "\n",
    "We can use mean/median replacement of missing values, or we can just drop the entire feature as a whole. These are very naive approaches are pretty terrible in actual application.\n",
    "\n",
    "- Better approach is we can use KNN techniques to find closer values in dimensional space works best with Numerical Data\n",
    "- For Categorical missing data we can use Deep Learning to predict these values. \n",
    "- Regression find linear and non-linear relationships between missing data. Most advanced techinique  MICE (Multiple Imputation by Chained Equations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MICE** is considered the best approach if getting more data is not plausible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Mice Full Breakdown**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🚀 What is MICE?\n",
    "MICE (**Multiple Imputation by Chained Equations**) is a smart way to **fill in missing values** in a dataset. Instead of just using an average or a guess, MICE **predicts the missing values** using patterns in the existing data.\n",
    "**MICE is not ideal for large datasets because it is computationally expensive, Best to use Deep Learning for Large Data**sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Psuedo Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " **1️⃣ First, Make an Initial Guess 🤔**\n",
    "- Imagine you have a table of students' test scores with some missing values:\n",
    "\n",
    "  | Student | Math | Science | English |\n",
    "  |---------|------|---------|---------|\n",
    "  | Alice   | 90   | **78 (guess)**   | 85      |\n",
    "  | Bob     | **87 (guess)** | 75      | 80      |\n",
    "  | Charlie | 85   | 80      | **82 (guess)**   |\n",
    "\n",
    "- MICE **fills in missing values with a rough guess** (e.g., using the average of the other values).\n",
    "\n",
    "---\n",
    "\n",
    "**2️⃣ Train a Model for Each Row 📊**\n",
    "- Now, MICE takes **one row at a time** and builds a mini **prediction model** using the other rows.\n",
    "- Example: If **Alice’s Science score is missing**, MICE **uses Math & English** for inference to predict Science.\n",
    "---\n",
    "\n",
    "**3️⃣ Predict Missing Values and Update the Table 🔄**\n",
    "- MICE **replaces the guessed values with better predictions** from the model.\n",
    "\n",
    "  | Student | Math | Science | English |\n",
    "  |---------|------|---------|---------|\n",
    "  | Alice   | 90   | **79 (predicted)** | 85  |\n",
    "  | Bob     | **87 (guess)** | 75 | 80  |\n",
    "  | Charlie | 85   | 80      | **82 (guess)** |\n",
    "\n",
    "---\n",
    "\n",
    "**4️⃣ Repeat Until Everything is Stable 🔁**\n",
    "- MICE **keeps improving the predictions** by repeating this process **multiple times** until the values **don’t change much**.\n",
    "- When this happens, we have a **complete dataset with no missing values**! 🎉\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🔥 Why is MICE Better Than Just Using an Average?\n",
    "✔ **Finds patterns in data** instead of making random guesses.  \n",
    "✔ **More accurate than mean/median imputation** because it considers relationships between variables.  \n",
    "✔ **Works well even when multiple features are missing.**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before MICE:\n",
      ",     Age   Salary  Experience\n",
      "0  25.0  50000.0         2.0\n",
      "1   NaN  60000.0         4.0\n",
      "2  30.0      NaN         6.0\n",
      "3  35.0  80000.0         NaN\n",
      "4  40.0  90000.0        10.0\n",
      "\n",
      "After MICE (Rounded & No Decimals):\n",
      "    Age  Salary  Experience\n",
      "0   25   50000           2\n",
      "1   27   60000           4\n",
      "2   30   69946           6\n",
      "3   35   80000           8\n",
      "4   40   90000          10\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.experimental import enable_iterative_imputer  # Enable MICE\n",
    "from sklearn.impute import IterativeImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample data with missing values\n",
    "data = {\n",
    "    'Age': [25, np.nan, 30, 35, 40], \n",
    "    'Salary': [50000, 60000, np.nan, 80000, 90000], \n",
    "    'Experience': [2, 4, 6, np.nan, 10]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data).round(2)  # Create DataFrame\n",
    "\n",
    "print(f\"Before MICE:\\n, {df}\")\n",
    "\n",
    "# Apply MICE to fill missing values\n",
    "imputer = IterativeImputer()  # Use default model (BayesianRidge)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "df_imputed = df_imputed.round(2)  # Round to 2 decimal places\n",
    "df_imputed = df_imputed.astype(int)  # Convert to whole numbers (removes decimals)\n",
    "\n",
    "print(\"\\nAfter MICE (Rounded & No Decimals):\\n\", df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to do with UnBalanced Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
